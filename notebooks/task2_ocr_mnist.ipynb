{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Handwritten Digit Recognition (OCR) with Neural Networks\n",
    "\n",
    "This notebook implements various neural network architectures for MNIST digit recognition, including feedforward ANNs and CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "print(f\"Pixel value range: {X_train.min()} to {X_train.max()}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_train))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample digits\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(20):\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.imshow(X_train[i], cmap='gray')\n",
    "    plt.title(f'Label: {y_train[i]}')\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Sample MNIST Digits')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Class distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "plt.bar(unique, counts)\n",
    "plt.xlabel('Digit Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('MNIST Training Set Class Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values to [0, 1]\n",
    "X_train_norm = X_train.astype('float32') / 255.0\n",
    "X_test_norm = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Reshape for feedforward neural network (flatten)\n",
    "X_train_flat = X_train_norm.reshape(X_train_norm.shape[0], -1)\n",
    "X_test_flat = X_test_norm.reshape(X_test_norm.shape[0], -1)\n",
    "\n",
    "# Keep original shape for CNN\n",
    "X_train_cnn = X_train_norm.reshape(X_train_norm.shape[0], 28, 28, 1)\n",
    "X_test_cnn = X_test_norm.reshape(X_test_norm.shape[0], 28, 28, 1)\n",
    "\n",
    "# Convert labels to categorical (one-hot encoding)\n",
    "y_train_cat = to_categorical(y_train, 10)\n",
    "y_test_cat = to_categorical(y_test, 10)\n",
    "\n",
    "print(f\"Flattened training data shape: {X_train_flat.shape}\")\n",
    "print(f\"CNN training data shape: {X_train_cnn.shape}\")\n",
    "print(f\"Categorical labels shape: {y_train_cat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Model - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline logistic regression\n",
    "baseline_lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "baseline_lr.fit(X_train_flat, y_train)\n",
    "\n",
    "y_pred_baseline = baseline_lr.predict(X_test_flat)\n",
    "baseline_accuracy = accuracy_score(y_test, y_pred_baseline)\n",
    "\n",
    "print(\"BASELINE Logistic Regression\")\n",
    "print(f\"Accuracy: {baseline_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_baseline, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Simple Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model A: Simple feedforward NN (128-64)\n",
    "model_a = keras.Sequential([\n",
    "    layers.Input(shape=(784,)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_a.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Model A Architecture:\")\n",
    "model_a.summary()\n",
    "\n",
    "# Train Model A\n",
    "history_a = model_a.fit(\n",
    "    X_train_flat, y_train_cat,\n",
    "    validation_data=(X_test_flat, y_test_cat),\n",
    "    epochs=15,\n",
    "    batch_size=128,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model B: Deeper network (256-128-64)\n",
    "model_b = keras.Sequential([\n",
    "    layers.Input(shape=(784,)),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_b.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Model B Architecture:\")\n",
    "model_b.summary()\n",
    "\n",
    "# Train Model B\n",
    "history_b = model_b.fit(\n",
    "    X_train_flat, y_train_cat,\n",
    "    validation_data=(X_test_flat, y_test_cat),\n",
    "    epochs=15,\n",
    "    batch_size=128,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Neural Network with Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model C: Network with Dropout\n",
    "model_c = keras.Sequential([\n",
    "    layers.Input(shape=(784,)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_c.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Model C Architecture:\")\n",
    "model_c.summary()\n",
    "\n",
    "# Train Model C\n",
    "history_c = model_c.fit(\n",
    "    X_train_flat, y_train_cat,\n",
    "    validation_data=(X_test_flat, y_test_cat),\n",
    "    epochs=15,\n",
    "    batch_size=128,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model D: Convolutional Neural Network\n",
    "model_d = keras.Sequential([\n",
    "    layers.Input(shape=(28, 28, 1)),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_d.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Model D (CNN) Architecture:\")\n",
    "model_d.summary()\n",
    "\n",
    "# Train Model D (CNN)\n",
    "history_d = model_d.fit(\n",
    "    X_train_cnn, y_train_cat,\n",
    "    validation_data=(X_test_cnn, y_test_cat),\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training histories\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Accuracy plots\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(history_a.history['accuracy'], label='Model A - Train')\n",
    "plt.plot(history_a.history['val_accuracy'], label='Model A - Val')\n",
    "plt.plot(history_b.history['accuracy'], label='Model B - Train')\n",
    "plt.plot(history_b.history['val_accuracy'], label='Model B - Val')\n",
    "plt.title('Feedforward Models - Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Loss plots\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(history_a.history['loss'], label='Model A - Train')\n",
    "plt.plot(history_a.history['val_loss'], label='Model A - Val')\n",
    "plt.plot(history_b.history['loss'], label='Model B - Train')\n",
    "plt.plot(history_b.history['val_loss'], label='Model B - Val')\n",
    "plt.title('Feedforward Models - Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Dropout model\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(history_c.history['accuracy'], label='Model C - Train')\n",
    "plt.plot(history_c.history['val_accuracy'], label='Model C - Val')\n",
    "plt.title('Model with Dropout - Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# CNN model\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(history_d.history['accuracy'], label='CNN - Train')\n",
    "plt.plot(history_d.history['val_accuracy'], label='CNN - Val')\n",
    "plt.title('CNN Model - Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "models = {\n",
    "    'Model A (128-64)': (model_a, X_test_flat),\n",
    "    'Model B (256-128-64)': (model_b, X_test_flat),\n",
    "    'Model C (Dropout)': (model_c, X_test_flat),\n",
    "    'Model D (CNN)': (model_d, X_test_cnn)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"=== MODEL EVALUATION RESULTS ===\")\n",
    "print(f\"Baseline (Logistic Regression): {baseline_accuracy:.4f}\")\n",
    "print()\n",
    "\n",
    "for name, (model, X_test_data) in models.items():\n",
    "    # Predictions\n",
    "    y_pred_proba = model.predict(X_test_data, verbose=0)\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[name] = accuracy\n",
    "    \n",
    "    print(f\"{name}: {accuracy:.4f}\")\n",
    "\n",
    "# Best model detailed evaluation\n",
    "best_model_name = max(results, key=results.get)\n",
    "best_model, best_X_test = models[best_model_name]\n",
    "\n",
    "print(f\"\\n=== BEST MODEL: {best_model_name} ===\")\n",
    "y_pred_best = np.argmax(best_model.predict(best_X_test, verbose=0), axis=1)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Final Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "model_names = ['Baseline\\n(LogReg)', 'Model A\\n(128-64)', 'Model B\\n(256-128-64)', \n",
    "               'Model C\\n(Dropout)', 'Model D\\n(CNN)']\n",
    "accuracies = [baseline_accuracy] + list(results.values())\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.bar(model_names, accuracies, color=['lightcoral', 'lightblue', 'lightgreen', 'lightyellow', 'lightpink'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('MNIST Digit Recognition - Model Comparison')\n",
    "plt.ylim(0.9, 1.0)\n",
    "\n",
    "# Add accuracy values on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "             f'{acc:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n=== FINAL SUMMARY ===\")\n",
    "print(f\"Best performing model: {best_model_name}\")\n",
    "print(f\"Best accuracy: {max(results.values()):.4f}\")\n",
    "print(f\"Improvement over baseline: {(max(results.values()) - baseline_accuracy)*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}