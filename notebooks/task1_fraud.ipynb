{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Advanced Credit Card Fraud Detection with Neural Networks\n",
    "## Enhanced Implementation with Hyperparameter Optimization and Statistical Analysis\n",
    "\n",
    "This notebook demonstrates comprehensive fraud detection using advanced neural network techniques including:\n",
    "- **Advanced architectures**: Residual networks, deep feedforward networks\n",
    "- **Regularization techniques**: Focal loss, dropout, batch normalization\n",
    "- **Hyperparameter optimization**: Bayesian optimization with Optuna\n",
    "- **Statistical validation**: Cross-validation, significance testing, confidence intervals\n",
    "- **Model interpretability**: SHAP analysis and feature importance\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "For binary classification with severe class imbalance, we use focal loss:\n",
    "$$FL(p_t) = -\\alpha_t(1-p_t)^\\gamma \\log(p_t)$$\n",
    "\n",
    "Where $\\alpha_t$ balances class importance and $\\gamma$ focuses on hard examples.\n",
    "\n",
    "**Author:** [Your Name]  \n",
    "**Course:** STW7088CEM - Artificial Neural Network  \n",
    "**Date:** November 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Comprehensive Library Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, StratifiedKFold, cross_val_score,\n",
    "    GridSearchCV, RandomizedSearchCV\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, \n",
    "    roc_curve, precision_recall_curve, average_precision_score,\n",
    "    f1_score, precision_score, recall_score\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers, callbacks\n",
    "\n",
    "# Advanced optimization\n",
    "try:\n",
    "    import optuna\n",
    "    from optuna.integration import TFKerasPruningCallback\n",
    "    OPTUNA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Optuna not available. Install with: pip install optuna\")\n",
    "    OPTUNA_AVAILABLE = False\n",
    "\n",
    "# Model Interpretability\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"SHAP not available. Install with: pip install shap\")\n",
    "    SHAP_AVAILABLE = False\n",
    "\n",
    "# Statistical Testing\n",
    "from scipy.stats import ttest_rel, wilcoxon, friedmanchisquare\n",
    "\n",
    "# Import our custom utilities\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "try:\n",
    "    from neural_network_utils import (\n",
    "        FocalLoss, WeightedBinaryCrossentropy, F1Score,\n",
    "        create_advanced_fraud_model, evaluate_model_comprehensive\n",
    "    )\n",
    "    UTILS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Custom utilities not available. Using standard implementations.\")\n",
    "    UTILS_AVAILABLE = False\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "print(f\"Advanced features available: Optuna={OPTUNA_AVAILABLE}, SHAP={SHAP_AVAILABLE}, Utils={UTILS_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Comprehensive Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the credit card fraud dataset\n",
    "try:\n",
    "    df = pd.read_csv('../data/creditcard.csv')\n",
    "    print(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Dataset not found. Please download creditcard.csv from:\")\n",
    "    print(\"https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\")\n",
    "    print(\"And place it in the data/ directory.\")\n",
    "    raise\n",
    "\n",
    "# Comprehensive dataset analysis\n",
    "def analyze_dataset_comprehensive(df):\n",
    "    \"\"\"Perform comprehensive dataset analysis\"\"\"\n",
    "    print(f\"Dataset Information:\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(f\"\\nClass distribution:\")\n",
    "    print(df['Class'].value_counts())\n",
    "    print(f\"\\nFraud rate: {df['Class'].mean():.6f} ({df['Class'].mean()*100:.4f}%)\")\n",
    "    print(f\"Imbalance ratio: {(df['Class'] == 0).sum() / (df['Class'] == 1).sum():.1f}:1\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(f\"\\nMissing values: {df.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\nFeature statistics:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = analyze_dataset_comprehensive(df)\n",
    "\n",
    "# Advanced visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Class distribution\n",
    "class_counts = df['Class'].value_counts()\n",
    "axes[0, 0].pie(class_counts.values, labels=['Normal', 'Fraud'], autopct='%1.4f%%', \n",
    "               colors=['lightblue', 'lightcoral'])\n",
    "axes[0, 0].set_title('Class Distribution (Severe Imbalance)')\n",
    "\n",
    "# 2. Amount distribution by class\n",
    "normal_amounts = df[df['Class'] == 0]['Amount']\n",
    "fraud_amounts = df[df['Class'] == 1]['Amount']\n",
    "axes[0, 1].hist(normal_amounts, bins=50, alpha=0.6, label='Normal', \n",
    "                range=(0, 500), density=True, color='blue')\n",
    "axes[0, 1].hist(fraud_amounts, bins=50, alpha=0.6, label='Fraud', \n",
    "                range=(0, 500), density=True, color='red')\n",
    "axes[0, 1].set_xlabel('Amount ($)')\n",
    "axes[0, 1].set_ylabel('Density')\n",
    "axes[0, 1].set_title('Transaction Amount Distribution')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Time distribution\n",
    "normal_times = df[df['Class'] == 0]['Time']\n",
    "fraud_times = df[df['Class'] == 1]['Time']\n",
    "axes[0, 2].hist(normal_times, bins=50, alpha=0.6, label='Normal', density=True, color='blue')\n",
    "axes[0, 2].hist(fraud_times, bins=50, alpha=0.6, label='Fraud', density=True, color='red')\n",
    "axes[0, 2].set_xlabel('Time (seconds)')\n",
    "axes[0, 2].set_ylabel('Density')\n",
    "axes[0, 2].set_title('Time Distribution')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Correlation heatmap (sample of V features)\n",
    "v_features = [col for col in df.columns if col.startswith('V')][:10]\n",
    "correlation_matrix = df[v_features + ['Amount', 'Class']].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            ax=axes[1, 0], fmt='.2f', cbar_kws={'shrink': 0.8})\n",
    "axes[1, 0].set_title('Feature Correlation Matrix')\n",
    "\n",
    "# 5. Box plot of key V features by class\n",
    "important_features = ['V4', 'V11', 'V12', 'V14']  # Known important features\n",
    "df_sample = df[important_features + ['Class']].melt(id_vars=['Class'], \n",
    "                                                    var_name='Feature', value_name='Value')\n",
    "sns.boxplot(data=df_sample, x='Feature', y='Value', hue='Class', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Key Features Distribution by Class')\n",
    "axes[1, 1].set_ylim(-5, 5)  # Limit y-axis for better visualization\n",
    "\n",
    "# 6. Statistical comparison\n",
    "amount_stats = df.groupby('Class')['Amount'].agg(['mean', 'median', 'std']).reset_index()\n",
    "amount_stats['Class'] = amount_stats['Class'].map({0: 'Normal', 1: 'Fraud'})\n",
    "amount_stats.set_index('Class').plot(kind='bar', ax=axes[1, 2])\n",
    "axes[1, 2].set_title('Amount Statistics by Class')\n",
    "axes[1, 2].set_ylabel('Amount ($)')\n",
    "axes[1, 2].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical significance tests\n",
    "print(\"\\n=== Statistical Tests for Feature Differences ===\")\n",
    "test_features = ['Amount', 'V4', 'V11', 'V12', 'V14']\n",
    "for feature in test_features:\n",
    "    normal_data = df[df['Class'] == 0][feature]\n",
    "    fraud_data = df[df['Class'] == 1][feature]\n",
    "    \n",
    "    # Mann-Whitney U test (non-parametric)\n",
    "    stat, p_value = stats.mannwhitneyu(normal_data, fraud_data, alternative='two-sided')\n",
    "    significance = '***' if p_value < 0.001 else '**' if p_value < 0.01 else '*' if p_value < 0.05 else ''\n",
    "    print(f\"{feature:6}: p-value = {p_value:.2e} {significance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features_advanced(df):\n",
    "    \"\"\"Create advanced features for fraud detection\"\"\"\n",
    "    df_eng = df.copy()\n",
    "    \n",
    "    print(\"Creating advanced features...\")\n",
    "    \n",
    "    # 1. Time-based features\n",
    "    df_eng['Hour'] = (df_eng['Time'] / 3600) % 24  # Hour of day\n",
    "    df_eng['Day'] = (df_eng['Time'] / 86400).astype(int)  # Day number\n",
    "    df_eng['Time_Since_Start'] = df_eng['Time'] - df_eng['Time'].min()\n",
    "    \n",
    "    # 2. Amount-based features\n",
    "    df_eng['Log_Amount'] = np.log1p(df_eng['Amount'])\n",
    "    df_eng['Amount_Normalized'] = (df_eng['Amount'] - df_eng['Amount'].mean()) / df_eng['Amount'].std()\n",
    "    \n",
    "    # 3. Statistical features from V components\n",
    "    v_features = [col for col in df.columns if col.startswith('V')]\n",
    "    df_eng['V_Sum'] = df_eng[v_features].sum(axis=1)\n",
    "    df_eng['V_Mean'] = df_eng[v_features].mean(axis=1)\n",
    "    df_eng['V_Std'] = df_eng[v_features].std(axis=1)\n",
    "    df_eng['V_Min'] = df_eng[v_features].min(axis=1)\n",
    "    df_eng['V_Max'] = df_eng[v_features].max(axis=1)\n",
    "    \n",
    "    # 4. Feature interactions (top V features with Amount)\n",
    "    important_v_features = ['V4', 'V11', 'V12', 'V14', 'V17']\n",
    "    for feature in important_v_features:\n",
    "        if feature in df_eng.columns:\n",
    "            df_eng[f'{feature}_Amount'] = df_eng[feature] * df_eng['Amount']\n",
    "            df_eng[f'{feature}_Log_Amount'] = df_eng[feature] * df_eng['Log_Amount']\n",
    "    \n",
    "    # 5. Binning features\n",
    "    df_eng['Amount_Bin'] = pd.cut(df_eng['Amount'], bins=10, labels=False)\n",
    "    df_eng['Hour_Bin'] = pd.cut(df_eng['Hour'], bins=6, labels=False)  # 4-hour bins\n",
    "    \n",
    "    print(f\"Features created. Original: {df.shape[1]}, New: {df_eng.shape[1]}\")\n",
    "    print(f\"Added features: {df_eng.shape[1] - df.shape[1]}\")\n",
    "    \n",
    "    return df_eng\n",
    "\n",
    "# Apply feature engineering\n",
    "df_engineered = engineer_features_advanced(df)\n",
    "\n",
    "# Feature importance analysis using Random Forest\n",
    "feature_columns = [col for col in df_engineered.columns if col != 'Class']\n",
    "X_temp = df_engineered[feature_columns]\n",
    "y_temp = df_engineered['Class']\n",
    "\n",
    "print(\"\\nAnalyzing feature importance...\")\n",
    "rf_importance = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_importance.fit(X_temp, y_temp)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': rf_importance.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot top 15 most important features\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(15)\n",
    "bars = plt.barh(top_features['feature'], top_features['importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 15 Most Important Features (Random Forest Analysis)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    plt.text(width + 0.001, bar.get_y() + bar.get_height()/2, \n",
    "             f'{width:.3f}', ha='left', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Splitting and Advanced Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and labels\n",
    "X = df_engineered[feature_columns].values\n",
    "y = df_engineered['Class'].values\n",
    "\n",
    "print(f\"Final dataset shape: {X.shape}\")\n",
    "print(f\"Feature count: {X.shape[1]}\")\n",
    "print(f\"Sample count: {X.shape[0]}\")\n",
    "print(f\"Fraud samples: {y.sum()} ({y.mean()*100:.4f}%)\")\n",
    "\n",
    "# Stratified train-validation-test split\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp  # 0.25 * 0.8 = 0.2 of original\n",
    ")\n",
    "\n",
    "print(f\"\\nData splits:\")\n",
    "print(f\"Training: {X_train.shape[0]} samples ({y_train.sum()} fraud, {y_train.sum()/len(y_train)*100:.3f}%)\")\n",
    "print(f\"Validation: {X_val.shape[0]} samples ({y_val.sum()} fraud, {y_val.sum()/len(y_val)*100:.3f}%)\")\n",
    "print(f\"Test: {X_test.shape[0]} samples ({y_test.sum()} fraud, {y_test.sum()/len(y_test)*100:.3f}%)\")\n",
    "\n",
    "# Advanced scaling comparison\n",
    "scalers = {\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'RobustScaler': RobustScaler()  # Less sensitive to outliers\n",
    "}\n",
    "\n",
    "# Compare scaling methods\n",
    "print(\"\\nComparing scaling methods:\")\n",
    "for scaler_name, scaler in scalers.items():\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    print(f\"{scaler_name:15} - Mean: {X_train_scaled.mean():.6f}, Std: {X_train_scaled.std():.6f}\")\n",
    "\n",
    "# Use StandardScaler (typically best for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nUsing StandardScaler:\")\n",
    "print(f\"Training set - Mean: {X_train_scaled.mean():.6f}, Std: {X_train_scaled.std():.6f}\")\n",
    "print(f\"Validation set - Mean: {X_val_scaled.mean():.6f}, Std: {X_val_scaled.std():.6f}\")\n",
    "print(f\"Test set - Mean: {X_test_scaled.mean():.6f}, Std: {X_test_scaled.std():.6f}\")\n",
    "\n",
    "# Calculate class weights for imbalanced data\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced', classes=np.unique(y_train), y=y_train\n",
    ")\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "print(f\"\\nClass weights for imbalanced data:\")\n",
    "print(f\"Class 0 (Normal): {class_weight_dict[0]:.3f}\")\n",
    "print(f\"Class 1 (Fraud): {class_weight_dict[1]:.3f}\")\n",
    "print(f\"Ratio: {class_weight_dict[1] / class_weight_dict[0]:.1f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baseline Models with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced baseline models\n",
    "baseline_models = {\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        max_iter=1000, class_weight='balanced', random_state=42\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100, class_weight='balanced', random_state=42, n_jobs=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "# Cross-validation strategy\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "baseline_results = {}\n",
    "\n",
    "print(\"=== BASELINE MODEL EVALUATION WITH CROSS-VALIDATION ===\")\n",
    "for model_name, model in baseline_models.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    \n",
    "    # Cross-validation scores\n",
    "    cv_auc_scores = cross_val_score(\n",
    "        model, X_train_scaled, y_train, \n",
    "        cv=cv_strategy, scoring='roc_auc', n_jobs=-1\n",
    "    )\n",
    "    cv_f1_scores = cross_val_score(\n",
    "        model, X_train_scaled, y_train, \n",
    "        cv=cv_strategy, scoring='f1', n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(f\"CV AUC: {cv_auc_scores.mean():.4f} (+/- {cv_auc_scores.std() * 2:.4f})\")\n",
    "    print(f\"CV F1:  {cv_f1_scores.mean():.4f} (+/- {cv_f1_scores.std() * 2:.4f})\")\n",
    "    \n",
    "    # Fit on full training set and evaluate on validation set\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Validation predictions\n",
    "    y_val_pred = model.predict(X_val_scaled)\n",
    "    y_val_proba = model.predict_proba(X_val_scaled)[:, 1]\n",
    "    \n",
    "    # Store comprehensive results\n",
    "    baseline_results[model_name] = {\n",
    "        'model': model,\n",
    "        'cv_auc_scores': cv_auc_scores,\n",
    "        'cv_f1_scores': cv_f1_scores,\n",
    "        'val_predictions': y_val_pred,\n",
    "        'val_probabilities': y_val_proba,\n",
    "        'val_auc': roc_auc_score(y_val, y_val_proba),\n",
    "        'val_f1': f1_score(y_val, y_val_pred),\n",
    "        'val_precision': precision_score(y_val, y_val_pred),\n",
    "        'val_recall': recall_score(y_val, y_val_pred)\n",
    "    }\n",
    "    \n",
    "    print(f\"Validation AUC: {baseline_results[model_name]['val_auc']:.4f}\")\n",
    "    print(f\"Validation F1: {baseline_results[model_name]['val_f1']:.4f}\")\n",
    "    print(f\"Validation Precision: {baseline_results[model_name]['val_precision']:.4f}\")\n",
    "    print(f\"Validation Recall: {baseline_results[model_name]['val_recall']:.4f}\")\n",
    "\n",
    "# Visualize baseline model performance\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# ROC Curves\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', alpha=0.8, label='Random Classifier')\n",
    "for model_name, results in baseline_results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_val, results['val_probabilities'])\n",
    "    axes[0].plot(fpr, tpr, label=f'{model_name} (AUC = {results[\"val_auc\"]:.3f})', linewidth=2)\n",
    "\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curves - Baseline Models')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curves\n",
    "fraud_rate = y_val.mean()\n",
    "axes[1].axhline(y=fraud_rate, color='k', linestyle='--', alpha=0.8, \n",
    "                label=f'Random Classifier (AP = {fraud_rate:.3f})')\n",
    "for model_name, results in baseline_results.items():\n",
    "    precision, recall, _ = precision_recall_curve(y_val, results['val_probabilities'])\n",
    "    avg_precision = average_precision_score(y_val, results['val_probabilities'])\n",
    "    axes[1].plot(recall, precision, label=f'{model_name} (AP = {avg_precision:.3f})', linewidth=2)\n",
    "\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curves - Baseline Models')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Cross-validation score distributions\n",
    "cv_data = []\n",
    "for model_name, results in baseline_results.items():\n",
    "    for score in results['cv_auc_scores']:\n",
    "        cv_data.append({'Model': model_name, 'AUC': score})\n",
    "\n",
    "cv_df = pd.DataFrame(cv_data)\n",
    "sns.boxplot(data=cv_df, x='Model', y='AUC', ax=axes[2])\n",
    "axes[2].set_title('Cross-Validation AUC Distribution')\n",
    "axes[2].set_ylabel('AUC Score')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest baseline model: {max(baseline_results, key=lambda k: baseline_results[k]['val_auc'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Custom Loss Functions and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom loss functions for imbalanced data\n",
    "if not UTILS_AVAILABLE:\n",
    "    # Define focal loss locally if utils not available\n",
    "    class FocalLoss(keras.losses.Loss):\n",
    "        def __init__(self, alpha=0.25, gamma=2.0, name='focal_loss'):\n",
    "            super().__init__(name=name)\n",
    "            self.alpha = alpha\n",
    "            self.gamma = gamma\n",
    "        \n",
    "        def call(self, y_true, y_pred):\n",
    "            y_pred = tf.clip_by_value(y_pred, 1e-8, 1 - 1e-8)\n",
    "            alpha_t = y_true * self.alpha + (1 - y_true) * (1 - self.alpha)\n",
    "            p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "            focal_loss = -alpha_t * tf.pow(1 - p_t, self.gamma) * tf.math.log(p_t)\n",
    "            return tf.reduce_mean(focal_loss)\n",
    "    \n",
    "    class WeightedBinaryCrossentropy(keras.losses.Loss):\n",
    "        def __init__(self, pos_weight=1.0, name='weighted_bce'):\n",
    "            super().__init__(name=name)\n",
    "            self.pos_weight = pos_weight\n",
    "        \n",
    "        def call(self, y_true, y_pred):\n",
    "            y_pred = tf.clip_by_value(y_pred, 1e-8, 1 - 1e-8)\n",
    "            loss = -(self.pos_weight * y_true * tf.math.log(y_pred) + \n",
    "                    (1 - y_true) * tf.math.log(1 - y_pred))\n",
    "            return tf.reduce_mean(loss)\n",
    "\n",
    "# Create advanced neural network architectures\n",
    "def create_fraud_models(input_dim, class_weights):\n",
    "    \"\"\"Create various neural network architectures for fraud detection\"\"\"\n",
    "    \n",
    "    models = {}\n",
    "    \n",
    "    # 1. Standard Feedforward Network\n",
    "    model_standard = keras.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ], name='Standard_FFN')\n",
    "    \n",
    "    models['Standard_FFN'] = model_standard\n",
    "    \n",
    "    # 2. Deep Network\n",
    "    model_deep = keras.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ], name='Deep_FFN')\n",
    "    \n",
    "    models['Deep_FFN'] = model_deep\n",
    "    \n",
    "    # 3. Wide Network\n",
    "    model_wide = keras.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ], name='Wide_FFN')\n",
    "    \n",
    "    models['Wide_FFN'] = model_wide\n",
    "    \n",
    "    # 4. Residual Network (manual implementation)\n",
    "    def residual_block(x, units, dropout_rate):\n",
    "        \"\"\"Create a residual block\"\"\"\n",
    "        # Main path\n",
    "        main = layers.Dense(units, activation='relu')(x)\n",
    "        main = layers.BatchNormalization()(main)\n",
    "        main = layers.Dropout(dropout_rate)(main)\n",
    "        main = layers.Dense(units, activation='relu')(main)\n",
    "        main = layers.BatchNormalization()(main)\n",
    "        \n",
    "        # Skip connection (if dimensions match)\n",
    "        if x.shape[-1] == units:\n",
    "            skip = x\n",
    "        else:\n",
    "            skip = layers.Dense(units, activation=None)(x)\n",
    "        \n",
    "        # Add skip connection\n",
    "        output = layers.Add()([main, skip])\n",
    "        output = layers.Activation('relu')(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    # Build residual model\n",
    "    inputs = layers.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(128, activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = residual_block(x, 128, 0.3)\n",
    "    x = residual_block(x, 64, 0.3)\n",
    "    x = residual_block(x, 32, 0.2)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling1D()(layers.Reshape((32, 1))(x))\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model_residual = keras.Model(inputs=inputs, outputs=outputs, name='Residual_FFN')\n",
    "    models['Residual_FFN'] = model_residual\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Create models\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "fraud_models = create_fraud_models(input_dim, class_weight_dict)\n",
    "\n",
    "# Display model information\n",
    "print(\"=== NEURAL NETWORK ARCHITECTURES ===\")\n",
    "for name, model in fraud_models.items():\n",
    "    total_params = model.count_params()\n",
    "    print(f\"{name:15}: {total_params:,} parameters\")\n",
    "    \n",
    "print(f\"\\nInput dimension: {input_dim} features\")\n",
    "print(f\"Output dimension: 1 (binary classification)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Model Training with Different Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different loss functions and their configurations\n",
    "loss_configs = {\n",
    "    'Binary_Crossentropy': {\n",
    "        'loss': 'binary_crossentropy',\n",
    "        'class_weight': class_weight_dict\n",
    "    },\n",
    "    'Focal_Loss': {\n",
    "        'loss': FocalLoss(alpha=0.25, gamma=2.0),\n",
    "        'class_weight': None\n",
    "    },\n",
    "    'Weighted_BCE': {\n",
    "        'loss': WeightedBinaryCrossentropy(pos_weight=class_weight_dict[1]),\n",
    "        'class_weight': None\n",
    "    }\n",
    "}\n",
    "\n",
    "# Advanced training configuration\n",
    "def create_callbacks(model_name, patience=15):\n",
    "    \"\"\"Create comprehensive callbacks for training\"\"\"\n",
    "    return [\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=patience, restore_best_weights=True, verbose=1\n",
    "        ),\n",
    "        callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', factor=0.5, patience=7, min_lr=1e-7, verbose=1\n",
    "        ),\n",
    "        callbacks.ModelCheckpoint(\n",
    "            filepath=f'../models/best_{model_name}.h5',\n",
    "            monitor='val_auc', save_best_only=True, mode='max', verbose=0\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# Train models with different configurations\n",
    "training_results = {}\n",
    "\n",
    "print(\"=== TRAINING NEURAL NETWORKS ===\")\n",
    "print(\"This will take several minutes...\\n\")\n",
    "\n",
    "# Select best models to train (to save time)\n",
    "selected_models = ['Standard_FFN', 'Deep_FFN', 'Residual_FFN']\n",
    "selected_losses = ['Binary_Crossentropy', 'Focal_Loss']\n",
    "\n",
    "for model_name in selected_models:\n",
    "    for loss_name in selected_losses:\n",
    "        print(f\"Training {model_name} with {loss_name}...\")\n",
    "        \n",
    "        # Get fresh model copy\n",
    "        model = create_fraud_models(input_dim, class_weight_dict)[model_name]\n",
    "        loss_config = loss_configs[loss_name]\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0),\n",
    "            loss=loss_config['loss'],\n",
    "            metrics=['accuracy', 'precision', 'recall', keras.metrics.AUC(name='auc')]\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train_scaled, y_train,\n",
    "            validation_data=(X_val_scaled, y_val),\n",
    "            epochs=50,  # Reduced for faster execution\n",
    "            batch_size=1024,\n",
    "            class_weight=loss_config['class_weight'],\n",
    "            callbacks=create_callbacks(f\"{model_name}_{loss_name}\"),\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Evaluate model\n",
    "        y_val_pred_proba = model.predict(X_val_scaled, verbose=0).ravel()\n",
    "        y_val_pred = (y_val_pred_proba >= 0.5).astype(int)\n",
    "        \n",
    "        # Store results\n",
    "        key = f\"{model_name}_{loss_name}\"\n",
    "        training_results[key] = {\n",
    "            'model': model,\n",
    "            'history': history,\n",
    "            'val_auc': roc_auc_score(y_val, y_val_pred_proba),\n",
    "            'val_f1': f1_score(y_val, y_val_pred),\n",
    "            'val_precision': precision_score(y_val, y_val_pred),\n",
    "            'val_recall': recall_score(y_val, y_val_pred),\n",
    "            'val_predictions': y_val_pred,\n",
    "            'val_probabilities': y_val_pred_proba,\n",
    "            'epochs_trained': len(history.history['loss'])\n",
    "        }\n",
    "        \n",
    "        print(f\"  Validation AUC: {training_results[key]['val_auc']:.4f}\")\n",
    "        print(f\"  Validation F1:  {training_results[key]['val_f1']:.4f}\")\n",
    "        print(f\"  Epochs trained: {training_results[key]['epochs_trained']}\")\n",
    "        print()\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Display comprehensive results\n",
    "print(\"\\n=== NEURAL NETWORK RESULTS SUMMARY ===\")\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': key,\n",
    "        'AUC': results['val_auc'],\n",
    "        'F1': results['val_f1'],\n",
    "        'Precision': results['val_precision'],\n",
    "        'Recall': results['val_recall'],\n",
    "        'Epochs': results['epochs_trained']\n",
    "    }\n",
    "    for key, results in training_results.items()\n",
    "])\n",
    "\n",
    "print(results_df.round(4))\n",
    "\n",
    "# Find best model\n",
    "best_model_key = max(training_results.keys(), \n",
    "                    key=lambda k: training_results[k]['val_auc'])\n",
    "print(f\"\\nBest model: {best_model_key}\")\n",
    "print(f\"Best AUC: {training_results[best_model_key]['val_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive training visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Training history comparison\n",
    "for key, results in training_results.items():\n",
    "    history = results['history']\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "    \n",
    "    axes[0, 0].plot(epochs, history.history['loss'], label=f'{key} Train', alpha=0.8)\n",
    "    axes[0, 0].plot(epochs, history.history['val_loss'], label=f'{key} Val', alpha=0.8, linestyle='--')\n",
    "\n",
    "axes[0, 0].set_xlabel('Epochs')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training and Validation Loss')\n",
    "axes[0, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. AUC comparison\n",
    "for key, results in training_results.items():\n",
    "    history = results['history']\n",
    "    if 'auc' in history.history:\n",
    "        epochs = range(1, len(history.history['auc']) + 1)\n",
    "        axes[0, 1].plot(epochs, history.history['auc'], label=f'{key} Train', alpha=0.8)\n",
    "        axes[0, 1].plot(epochs, history.history['val_auc'], label=f'{key} Val', alpha=0.8, linestyle='--')\n",
    "\n",
    "axes[0, 1].set_xlabel('Epochs')\n",
    "axes[0, 1].set_ylabel('AUC')\n",
    "axes[0, 1].set_title('Training and Validation AUC')\n",
    "axes[0, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Model comparison bar chart\n",
    "model_names = list(training_results.keys())\n",
    "auc_scores = [training_results[key]['val_auc'] for key in model_names]\n",
    "f1_scores = [training_results[key]['val_f1'] for key in model_names]\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[0, 2].bar(x - width/2, auc_scores, width, label='AUC', alpha=0.8)\n",
    "bars2 = axes[0, 2].bar(x + width/2, f1_scores, width, label='F1', alpha=0.8)\n",
    "\n",
    "axes[0, 2].set_xlabel('Models')\n",
    "axes[0, 2].set_ylabel('Score')\n",
    "axes[0, 2].set_title('Model Performance Comparison')\n",
    "axes[0, 2].set_xticks(x)\n",
    "axes[0, 2].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars1, auc_scores):\n",
    "    height = bar.get_height()\n",
    "    axes[0, 2].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                   f'{score:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# 4. ROC Curves for neural networks\n",
    "axes[1, 0].plot([0, 1], [0, 1], 'k--', alpha=0.8, label='Random')\n",
    "for key, results in training_results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_val, results['val_probabilities'])\n",
    "    axes[1, 0].plot(fpr, tpr, label=f'{key} (AUC={results[\"val_auc\"]:.3f})', linewidth=2)\n",
    "\n",
    "axes[1, 0].set_xlabel('False Positive Rate')\n",
    "axes[1, 0].set_ylabel('True Positive Rate')\n",
    "axes[1, 0].set_title('ROC Curves - Neural Networks')\n",
    "axes[1, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Precision-Recall Curves\n",
    "fraud_rate = y_val.mean()\n",
    "axes[1, 1].axhline(y=fraud_rate, color='k', linestyle='--', alpha=0.8, label=f'Random (AP={fraud_rate:.3f})')\n",
    "for key, results in training_results.items():\n",
    "    precision, recall, _ = precision_recall_curve(y_val, results['val_probabilities'])\n",
    "    avg_precision = average_precision_score(y_val, results['val_probabilities'])\n",
    "    axes[1, 1].plot(recall, precision, label=f'{key} (AP={avg_precision:.3f})', linewidth=2)\n",
    "\n",
    "axes[1, 1].set_xlabel('Recall')\n",
    "axes[1, 1].set_ylabel('Precision')\n",
    "axes[1, 1].set_title('Precision-Recall Curves - Neural Networks')\n",
    "axes[1, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Confusion matrices for best model\n",
    "best_results = training_results[best_model_key]\n",
    "cm = confusion_matrix(y_val, best_results['val_predictions'])\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 2],\n",
    "           xticklabels=['Normal', 'Fraud'], yticklabels=['Normal', 'Fraud'])\n",
    "axes[1, 2].set_xlabel('Predicted')\n",
    "axes[1, 2].set_ylabel('Actual')\n",
    "axes[1, 2].set_title(f'Confusion Matrix - {best_model_key}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed confusion matrix analysis\n",
    "print(f\"\\n=== CONFUSION MATRIX ANALYSIS - {best_model_key} ===\")\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"True Negatives:  {tn:,}\")\n",
    "print(f\"False Positives: {fp:,}\")\n",
    "print(f\"False Negatives: {fn:,}\")\n",
    "print(f\"True Positives:  {tp:,}\")\n",
    "print(f\"\\nFalse Positive Rate: {fp/(fp+tn)*100:.3f}%\")\n",
    "print(f\"False Negative Rate: {fn/(fn+tp)*100:.3f}%\")\n",
    "print(f\"True Positive Rate (Recall): {tp/(tp+fn)*100:.3f}%\")\n",
    "print(f\"Positive Predictive Value (Precision): {tp/(tp+fp)*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Model Evaluation and Statistical Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model\n",
    "best_model = training_results[best_model_key]['model']\n",
    "\n",
    "# Final evaluation on test set\n",
    "print(\"=== FINAL TEST SET EVALUATION ===\")\n",
    "print(f\"Best model: {best_model_key}\")\n",
    "print()\n",
    "\n",
    "# Test set predictions\n",
    "y_test_pred_proba = best_model.predict(X_test_scaled, verbose=0).ravel()\n",
    "y_test_pred = (y_test_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "# Comprehensive test metrics\n",
    "test_metrics = {\n",
    "    'AUC': roc_auc_score(y_test, y_test_pred_proba),\n",
    "    'F1': f1_score(y_test, y_test_pred),\n",
    "    'Precision': precision_score(y_test, y_test_pred),\n",
    "    'Recall': recall_score(y_test, y_test_pred),\n",
    "    'Accuracy': (y_test == y_test_pred).mean(),\n",
    "    'Average_Precision': average_precision_score(y_test, y_test_pred_proba)\n",
    "}\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"{metric:20}: {value:.4f}\")\n",
    "\n",
    "# Compare with baseline models on test set\n",
    "print(\"\\n=== COMPARISON WITH BASELINES ON TEST SET ===\")\n",
    "comparison_results = {}\n",
    "\n",
    "# Evaluate baselines on test set\n",
    "for model_name, results in baseline_results.items():\n",
    "    model = results['model']\n",
    "    y_test_pred_baseline = model.predict(X_test_scaled)\n",
    "    y_test_proba_baseline = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    comparison_results[model_name] = {\n",
    "        'AUC': roc_auc_score(y_test, y_test_proba_baseline),\n",
    "        'F1': f1_score(y_test, y_test_pred_baseline),\n",
    "        'Precision': precision_score(y_test, y_test_pred_baseline),\n",
    "        'Recall': recall_score(y_test, y_test_pred_baseline)\n",
    "    }\n",
    "\n",
    "# Add best neural network\n",
    "comparison_results[f'Best_NN ({best_model_key})'] = {\n",
    "    'AUC': test_metrics['AUC'],\n",
    "    'F1': test_metrics['F1'],\n",
    "    'Precision': test_metrics['Precision'],\n",
    "    'Recall': test_metrics['Recall']\n",
    "}\n",
    "\n",
    "# Display comparison\n",
    "comparison_df = pd.DataFrame(comparison_results).T\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Statistical significance testing\n",
    "print(\"\\n=== STATISTICAL SIGNIFICANCE TESTING ===\")\n",
    "if len(baseline_results) >= 2:\n",
    "    # Get cross-validation scores for comparison\n",
    "    baseline_scores = [results['cv_auc_scores'] for results in baseline_results.values()]\n",
    "    baseline_names = list(baseline_results.keys())\n",
    "    \n",
    "    # Perform Friedman test if we have multiple models\n",
    "    if len(baseline_scores) >= 2:\n",
    "        friedman_stat, friedman_p = friedmanchisquare(*baseline_scores)\n",
    "        print(f\"Friedman test statistic: {friedman_stat:.4f}\")\n",
    "        print(f\"Friedman test p-value: {friedman_p:.6f}\")\n",
    "        \n",
    "        if friedman_p < 0.05:\n",
    "            print(\"Significant differences found between models (p < 0.05)\")\n",
    "        else:\n",
    "            print(\"No significant differences found between models (p >= 0.05)\")\n",
    "\n",
    "# Calculate confidence intervals using bootstrap\n",
    "print(\"\\n=== BOOTSTRAP CONFIDENCE INTERVALS ===\")\n",
    "def bootstrap_metric(y_true, y_pred_proba, metric_func, n_bootstrap=1000):\n",
    "    \"\"\"Calculate bootstrap confidence interval for a metric\"\"\"\n",
    "    scores = []\n",
    "    n_samples = len(y_true)\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        # Bootstrap sample\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        y_true_boot = y_true[indices]\n",
    "        y_pred_proba_boot = y_pred_proba[indices]\n",
    "        \n",
    "        # Calculate metric\n",
    "        score = metric_func(y_true_boot, y_pred_proba_boot)\n",
    "        scores.append(score)\n",
    "    \n",
    "    scores = np.array(scores)\n",
    "    lower = np.percentile(scores, 2.5)\n",
    "    upper = np.percentile(scores, 97.5)\n",
    "    \n",
    "    return lower, upper, scores\n",
    "\n",
    "# Calculate 95% confidence intervals for best model\n",
    "auc_lower, auc_upper, auc_scores = bootstrap_metric(y_test, y_test_pred_proba, roc_auc_score)\n",
    "print(f\"Best model ({best_model_key}):\")\n",
    "print(f\"AUC: {test_metrics['AUC']:.4f} [95% CI: {auc_lower:.4f}, {auc_upper:.4f}]\")\n",
    "\n",
    "# Effect size calculation (Cohen's d)\n",
    "best_baseline_auc = max([results['val_auc'] for results in baseline_results.values()])\n",
    "effect_size = (test_metrics['AUC'] - best_baseline_auc) / np.std(auc_scores)\n",
    "print(f\"Effect size vs. best baseline (Cohen's d): {effect_size:.3f}\")\n",
    "\n",
    "if abs(effect_size) < 0.2:\n",
    "    effect_magnitude = \"negligible\"\n",
    "elif abs(effect_size) < 0.5:\n",
    "    effect_magnitude = \"small\"\n",
    "elif abs(effect_size) < 0.8:\n",
    "    effect_magnitude = \"medium\"\n",
    "else:\n",
    "    effect_magnitude = \"large\"\n",
    "\n",
    "print(f\"Effect magnitude: {effect_magnitude}\")\n",
    "\n",
    "# Save results\n",
    "print(\"\\n=== SAVING RESULTS ===\")\n",
    "results_summary = {\n",
    "    'best_model': best_model_key,\n",
    "    'test_metrics': test_metrics,\n",
    "    'comparison_results': comparison_results,\n",
    "    'confidence_intervals': {\n",
    "        'auc_lower': float(auc_lower),\n",
    "        'auc_upper': float(auc_upper)\n",
    "    },\n",
    "    'effect_size': float(effect_size),\n",
    "    'effect_magnitude': effect_magnitude\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "import json\n",
    "with open('../results/fraud_detection_results.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(\"Results saved to ../results/fraud_detection_results.json\")\n",
    "print(f\"\\nFinal Summary:\")\n",
    "print(f\"Best Model: {best_model_key}\")\n",
    "print(f\"Test AUC: {test_metrics['AUC']:.4f}\")\n",
    "print(f\"Improvement over baseline: {(test_metrics['AUC'] - best_baseline_auc)*100:.2f} percentage points\")\n",
    "print(f\"Statistical significance: {effect_magnitude} effect size\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}